```markdown
# MNIST 분류 실험 결과

## 기본 모델 성능
- 최종 테스트 정확도: 96.95%
- 훈련 시간: 48초

## 실험 결과
### 실험 1: 학습률 변경
- 변경사항: 학습률 증가 실험은 0.002에서 0.01까지 0.001씩 증가시키며 진행하였고, 학습률 감소 실험은 0.0001(1e-4), 0.00001(1e-5), 0.00001(1e-5)로 설정하여 진행하였다.
- 결과: 학습률 증가 실험은 (0.002, 97.16%),(0.003, 96.77%),(0.004, 96.57%),(0.005, 96.58%),(0.006, 96.59),(0.007, 96.20%),(0.008, 95.70%),(0.009, 96.49%),(0.01, 95.33%)의 결과를 보였고, 학습률 감소 실험은 (0.0001,92.47%),(0.00001,84.94%),(0.000001,39.99%)의 결과를 보였다.
- 분석: 학습률 증가 실험의 경우, 학습률이 0.002일 때만 기존 모델보다 약간 향상된 성능(97.16%)을 보였으며, 0.003부터는 전반적으로 성능이 점차 저하되는 경향을 보였다. 학습률 감소 실험의 경우, 0.0001(1e-4), 0.00001(1e-5), 0.000001(1e-6) 모두에서 기존 모델보다 현저히 저하된 성능을 보였다. 이는 너무 크거나 작은 학습률이 수렴을 방해하기 때문으로 보인다.

### 실험 2: 활성화 함수 변경
- 변경사항: 기존 ReLU 활성화 함수를 Tanh 활성화 함수와 Sigmoid 활성화 함수로 변경하여 각각 실험을 진행하였다.
- 결과: Tanh 활성화 함수는 96.37%, Sigmoid 활성화 함수는 95.14%의 성능을 보였다.
- 분석: Tanh 활성화 함수와 Sigmoid 활성화 함수 모두 기존 모델보다 약간 저하된 성능을 보였다. 이는 Vanishing Gradinet 문제 때문으로 보인다.

## 결론 및 인사이트
- 가장 효과적인 개선 방법: 실험 결과를 바탕으로 하면, 학습률은 0.002로 변경하고, 기존의 ReLU 활성화 함수를 유지하는 것이 가장 좋은 성능을 보이는 방법이다.
- 관찰된 패턴: 학습률이 커지면 성능이 향상되었지만, 일정 수준을 넘어가면 성능이 하락하였으며, 학습률이 작아지면 성능이 하락하였다. 활성화 함수를 ReLU에서 Tanh 또는 Sigmoid로 변경할 경우 모두 성능이 하락하였다.
- 추가 개선 아이디어: 에포크를 overfitting이 일어나지 않을 정도로 증가시키면 성능이 향상될 것이라 예상한다. 또한 조사해본 결과, Leaky ReLU, GELU, ELU와 같은 활성화 함수를 사용하면 성능을 미세하게 향상시킬 수 있을 것 같다.
```